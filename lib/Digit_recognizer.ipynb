{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80414d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44817c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Test/Training data\n",
    "(X_train_input, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Normalizee Data to be between 1 and 0\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0 # Flatten and normalize\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0 # Flatten and normalize\n",
    "\n",
    "# Matrix of Training Data\n",
    "m,n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_():\n",
    "  hidden_units = 128  \n",
    "  W1 = np.random.rand(784, hidden_units) - 0.5  \n",
    "  b1 = np.random.rand(hidden_units, 1)\n",
    "  W2 = np.random.rand(10, hidden_units) - 0.5\n",
    "  b2 = np.random.rand(10, 1)\n",
    "  lr = 0.1  # Learning Rate\n",
    "  return (W1, W2, b1, b2, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a69af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \n",
    "def ReLu(Z):\n",
    "  return(np.maximum(Z,0))\n",
    "\n",
    "def SoftMax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) \n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def derv_ReLu(Z):\n",
    "  return(Z > 0)\n",
    "  \n",
    "def one_hot(Y):\n",
    "  one_hot_Y = np.zeros((Y.size, Y.max() + 1))  # For batch input\n",
    "  one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "  one_hot_Y = one_hot_Y.T  # Transpose to shape (10, n) \n",
    "  return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function derivatives\n",
    "def derv_ReLu(X):\n",
    "    return X >= 0\n",
    "\n",
    "def derv_loss(observed, predicted):\n",
    "    # SoftMax and Cross-entropy loss\n",
    "    return predicted - observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3339c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation\n",
    "def fowardprop(W1, W2, b1, b2, X):\n",
    "  Z1 = np.matmul(X, W1) + b1.T  # Shape (1, hidden_units)\n",
    "  A1 = ReLu(Z1)\n",
    "  Z2 = np.matmul(A1, W2.T) + b2.T  # Shape (1, 10)\n",
    "  A2 = SoftMax(Z2)\n",
    "  return (A2, A1, Z1)\n",
    "\n",
    "def backprop(W2, A1, A2, Z1, Y, input):\n",
    "  # X: Predicted value\n",
    "  # Y: Actual value\n",
    "  n = int(A1.shape[0])\n",
    "  Y_one_hot = one_hot(Y)\n",
    "  \n",
    "  # Second Layer\n",
    "  dZ2 = derv_loss(Y_one_hot, A2.T)  # Shape (samples, 10)\n",
    "  dw2 = dZ2.dot(A1) / n # Shape (10, hidden_units)\n",
    "  db2 = np.sum(dZ2.T, axis=0, keepdims=True) / n # Shape (1, 10)\n",
    "\n",
    "  # First Layer\n",
    "  dZ1 = dZ2.T.dot(W2) * derv_ReLu(Z1)  # Shape (1, hidden_units)\n",
    "  dw1 = input.T.dot(dZ1) / n  # Shape (784, hidden_units)\n",
    "  db1 = np.sum(dZ1, axis=0, keepdims=True) / n  # Shape (1, hidden_units)\n",
    "\n",
    "  return(dw1, dw2, db1, db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f076a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Parameters\n",
    "def Update(W1, W2, b1, b2, dw1, dw2, db1, db2, lr):\n",
    "  new_W1 = W1 - lr * dw1\n",
    "  new_W2 = W2 - lr * dw2\n",
    "  new_b1 = b1 - lr * db1.T\n",
    "  new_b2 = b2 - lr * db2.T\n",
    "  return(new_W1, new_W2, new_b1, new_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Calculations\n",
    "def Get_pred(A2):\n",
    "  return(np.argmax(A2, axis=1))\n",
    "\n",
    "def Pred_accuracy(predictions, test_data):\n",
    "  print(predictions, test_data)\n",
    "  return((np.sum(predictions == test_data) / test_data.size) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Decent\n",
    "def Gradient_decent(train_data, train_labels, iterations):\n",
    "  W1, W2, b1, b2, lr= _init_()\n",
    "  for i in range(iterations):\n",
    "    prediction = []\n",
    "    A2, A1, Z1 = fowardprop(W1, W2, b1, b2, train_data)\n",
    "    dw1, dw2, db1, db2 = backprop(W2, A1, A2, Z1, train_labels, train_data)\n",
    "    W1, W2, b1, b2 = Update(W1, W2, b1, b2, dw1, dw2, db1, db2, lr)\n",
    "    if (i % 10 == 0):\n",
    "      print(f\"Iteration: {i}\")\n",
    "      prediction = np.append(prediction, Get_pred(A2))\n",
    "      print(f\"Accuracy: {Pred_accuracy(prediction, train_labels):.2f}%\")\n",
    "  return(W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541eb07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (60000,784) and (700,10) not aligned: 784 (dim 1) != 700 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Running Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m W1, W2, b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mGradient_decent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Testing the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m Get_pred(fowardprop(W1, W2, b1, b2, X_test)[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[47], line 19\u001b[0m, in \u001b[0;36mGradient_decent\u001b[1;34m(train_data, train_labels, iterations)\u001b[0m\n\u001b[0;32m     17\u001b[0m W1, W2, b1, b2, lr \u001b[38;5;241m=\u001b[39m _init_()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m---> 19\u001b[0m   A2, A1, Z2, Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mfowardprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m   dw1, dw2, db1, db2 \u001b[38;5;241m=\u001b[39m backprop(W1, W2, A1, A2, train_labels, train_data)\n\u001b[0;32m     21\u001b[0m   W1, W2, b1, b2 \u001b[38;5;241m=\u001b[39m Update(W1, W2, b1, b2, dw1, dw2, db1, db2, lr)\n",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m, in \u001b[0;36mfowardprop\u001b[1;34m(W1, W2, b1, b2, X)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfowardprop\u001b[39m(W1, W2, b1, b2, X):\n\u001b[1;32m----> 3\u001b[0m   Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m      4\u001b[0m   A1 \u001b[38;5;241m=\u001b[39m ReLu(Z1)\n\u001b[0;32m      5\u001b[0m   Z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(A1, W2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m b2\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (60000,784) and (700,10) not aligned: 784 (dim 1) != 700 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Running Model\n",
    "W1, W2, b1, b2 = Gradient_decent(X_train, Y_train, 500)\n",
    " \n",
    "# Testing the model\n",
    "test_predictions = Get_pred(fowardprop(W1, W2, b1, b2, X_test)[0])\n",
    "test_accuracy = Pred_accuracy(test_predictions, Y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
